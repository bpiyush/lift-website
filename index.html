<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Chirality in Action: Time-Aware Video Representation Learning by Latent Straightening">
  <meta name="keywords" content="Video representation learning, temporal understanding, action recognition, temporal modeling, time-aware representation learning">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Chirality in Action</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <!-- <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script> -->

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <style>
    .video-carousel-container {
      display: flex;
      align-items: center;
      width: 100%;
    }
    
    .video-carousel {
      flex: 1;
      position: relative;
      padding: 0;
    }
    
    .carousel-btn {
      background: #6b7280;
      color: white;
      border: none;
      padding: 12px 16px;
      font-size: 20px;
      cursor: pointer;
      border-radius: 50%;
      transition: all 0.3s ease;
      box-shadow: 0 4px 8px rgba(0, 0, 0, 0.2);
      min-width: 48px;
      height: 48px;
      display: flex;
      align-items: center;
      justify-content: center;
    }
    
    .carousel-btn:hover {
      background: #4b5563;
      box-shadow: 0 6px 12px rgba(0, 0, 0, 0.3);
      transform: translateY(-2px);
    }
    
    .carousel-btn:active {
      transform: translateY(0);
      box-shadow: 0 2px 4px rgba(0, 0, 0, 0.2);
    }
    
    #carousel-video, #dino-carousel-video {
      height: 400px;
      object-fit: contain;
      width: 100%;
    }
    
    #qual-carousel-image {
      height: 400px;
      object-fit: contain;
      width: 100%;
    }
  </style>
    /* No custom model controls needed: using native video controls */
  <!-- <link rel="icon" href="./static/images/favicon.svg"> -->
  <!-- <link rel="icon" href="./static/images/uva.png"> -->
  <!-- <link rel="icon" href="./static/images/oxford-logo.png"> -->
  <link rel="icon" href="./static/images/vgg-logo.png">

  <!-- Add copy button CSS and JS -->
  <link rel="stylesheet" type="text/css" href="./static/css/copy_button.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <!-- <script src="./static/js/index.js"></script> -->
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://www.robots.ox.ac.uk/~vgg/publications/">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://www.robots.ox.ac.uk/~vgg/publications/">
            VGG, Oxford
          </a>
        </div>
      </div>
    </div>

  </div>
</nav>


<!-- Title section -->
<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <img src="./assets/clock-time.gif" width="70"/>
          <h1 class="title is-1 publication-title">
            <!-- &#129371;  -->
            <!-- <img src="./assets/clock-time.gif" width="70"/> -->
            Chirality in Action: Time-Aware Video Representation Learning by Latent Straightening
          </h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://bpiyush.github.io">Piyush Bagad</a>, &nbsp;
            </span>
            <span class="author-block">
              <a href="https://www.robots.ox.ac.uk/~az/">Andrew Zisserman</a>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">University of Oxford  &nbsp;  </span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <!-- <span class="link-block">
                <a href="#"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Camera-ready</span>
                </a>
              </span> -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2509.08502"
                   class="external-link button is-normal is-rounded is-dark">
                  <!-- <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span> -->
                  <span>üìã arXiv</span>
                </a>
              </span>
            </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="#"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code (Coming Soon)</span>
                  </a>
              </span>
              <span class="link-block">
                <a href="https://huggingface.co/datasets/bpiyush/chirality-in-action"
                   class="external-link button is-normal is-rounded is-dark">
                  <!-- <span class="icon">
                    <i class="fas fa-database"></i>
                  </span> -->
                  <span>ü§ó Data</span>
                  </a>
              </span>
              <!-- <span class="link-block">
                <a href="#"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-chalkboard"></i>
                  </span>
                  <span>Poster</span>
                </a>
              </span> -->
              <!-- Video Link. -->
                <!-- <span class="link-block">
                  <a href="#"
                     class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                        <i class="fab fa-youtube"></i>
                    </span>
                    <span>Video</span>
                  </a>
                </span>   -->
                <span class="link-block">
                  <a href="#"
                     class="external-link button is-normal is-rounded is-dark">
                    <!-- <span class="icon">
                        <i class="fas fa-chalkboard"></i>
                    </span> -->
                    <span>ü§ó Models (Coming Soon)</span>
                  </a>
                </span>    
                <!-- Demo -->
                <!-- <span class="link-block">
                  <a href="https://huggingface.co/spaces/bpiyush/SoundOfWater"
                     class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                        <i class="fas fa-chalkboard"></i>
                    </span>
                    <span>Demo</span>
                  </a>
                </span>   -->
                <!-- ICASSP Poster -->
                <span class="link-block">
                  <a href="./assets/icassp-poster-v3-5.pdf"
                     class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                        <i class="fas fa-file-powerpoint"></i>
                    </span>
                    <span>Poster (NeurIPS 2025)</span>
                  </a>
                </span> 
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <span>
        <b>Chiral actions</b>: Our objective is to learn a video embedding that encodes the direction of time
        such that it is able to linearly separate temporally opposite (chiral) actions.
      </span>
      <div class="video-carousel-container">
        <button class="carousel-btn prev-btn" onclick="previousVideo()">‚ùÆ</button>
        <div class="video-carousel">
          <video id="carousel-video" autoplay muted>
            <source src="./assets/climb_up_down.mov" type="video/mp4">
          </video>
        </div>
        <button class="carousel-btn next-btn" onclick="nextVideo()">‚ùØ</button>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <h2 class="title is-4">üîê The Key Nugget</h2>
    <span>
      <b>Key observation</b>: tSNE projections of per-frame features from DINOv2 show that they lie on a time-sensitive trajectory. Can we
      use these to learn a time-aware video representation?
    </span>
    <div class="hero-body">
      <div class="video-carousel-container">
        <button class="carousel-btn prev-btn" onclick="previousDinoVideo()">‚ùÆ</button>
        <div class="video-carousel">
          <video id="dino-carousel-video" autoplay muted>
            <source src="./assets/dino_trajectory-1.mp4" type="video/mp4">
          </video>
        </div>
        <button class="carousel-btn next-btn" onclick="nextDinoVideo()">‚ùØ</button>
      </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <h2 class="title is-4">üß† The Perceptual Straightening Hypothesis</h2>
    <span>
      Henaff et al. (2019) hypothesized that humans convert non-linear spatial representations of naturally 
      occurring videos into linear temporal trajectories enabling their prediction with linear extrapolation.
      We are loosely inspired by this idea to transform DINO trajectories into a time-aware video embedding
      under a linearised Auto-Encoder model.
    </span>
    <div class="hero-body">
      <img src="./assets/psh.png" width="100%"/>
      <span> <small>[1] Perceptual straightening of natural videos. Olivier J. H√©naff, Robbe L. T. Goris and Eero P. Simoncelli. Nature 2019.</small></span>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <h2 class="title is-4">üèóÔ∏è The Model: LiFT</h2>
      <div class="hero-body">
        Please play the video animation below to understand the LiFT model design.
        <video id="model-explainer-video" controls muted playsinline loop style="width:100%; height:auto;">
          <source src="./assets/lift-architecture-explainer1-ezgif.com-crop-video.mp4" type="video/mp4">
        </video>
        <!-- <div><a href="./assets/lift-architecture-explainer-compressed.mov" download>Download video</a></div> -->
      <!-- Add a list with bullet points -->
      <div class="content">
        <ul>
          <li>
            LiFT is trained in an unsupervised manner by reconstructing the input feature sequence.
          </li>
          <li>
            What does LiFT learn? We observe that it essentially learns a smooth approximation of 
            the feature trajectory. Furthermore, it is able to learn different embeddings for
            videos of opening vs closing door actions. See the qualitative results below.
          </li>
        </ul>
      </div>
      <!-- Add a carousel to show three qual images -->
      <div class="video-carousel-container">
        <button class="carousel-btn prev-btn" onclick="previousQualImage()">‚ùÆ</button>
        <div class="video-carousel">
          <img id="qual-carousel-image" src="./assets/qual-result-1.png" alt="Qualitative Result" style="width: 100%; height: 400px; object-fit: contain;">
        </div>
        <button class="carousel-btn next-btn" onclick="nextQualImage()">‚ùØ</button>
      </div>
    </div>
  </div>
</section>


<section class="hero teaser">
  <div class="container is-max-desktop">
    <h2 class="title is-4">üéûÔ∏è The Chirality in Action Benchmark</h2>
    <span>
      We repurpose three existing datasets (SSv2, EPIC, Charades) to mine chiral actions
      and build a new benchmark to probe video embedding models for chirality. We search
      for temporally opposite verbs using ChatGPT and then group together similar nouns
      to construct chiral groups.
    </span>
    <div class="hero-body">
      <img src="./assets/cia-snap.png" width="85%"/>
    </div>
    <span><b>Evaluation protocol:</b> For each chiral group, we compute video embeddings for + and - samples. Then, we train a linear probe. The overall accuracy is averaged across all chiral groups.</span>
    
  </div>
</section>


<section class="section" id="acknowledgements">
  <div class="container is-max-desktop content">
    <h3 class="title">üôè Acknowledgements</h3>
    <ul>
      <li>
        We thank <a href="https://www.cs.ox.ac.uk/people/ashish.thandavan/">Ashish Thandavan</a> 
        for support with infrastructure and
        <a href="https://sindhu-hegde.github.io/">Sindhu Hegde</a>,
        <!-- <a href="https://ragavsachdeva.github.io/">Ragav Sachdeva</a>, -->
        <!-- <a href="https://www.robots.ox.ac.uk/~jaesung/">Jaesung Huh</a>, -->
        <!-- <a href="https://v-iashin.github.io/">Vladimir Iashin</a>,  -->
        <!-- <a href="https://www.robots.ox.ac.uk/~prajwal/">Prajwal KR</a>, -->
        <a href="https://makarandtapaswi.github.io/">Makarand Tapaswi</a>,
        <!-- and 
        <a href="https://rodosingh.github.io/">Aditya Singh</a> for useful discussions. -->
      </li>
      <!-- <li>
        We thank <a href="https://www.cs.unc.edu/~wilson/">Justin Wilson</a> for their 
        encouragement and help with details regarding dataset proposed in 
        <a href="https://gamma.cs.unc.edu/PSNN/">Wilson et al.</a> (2019).
      </li> -->
      <li>
        This research is funded by the EPSRC Programme Grant VisualAI EP/T028572/1, and a Royal Society Research Professorship RSRP\R\241003</li>
      </li>
    </ul>
  </div>
</section>

<section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">üìú Citation</h2>
      <pre><code id="code-block-article">
      @article{bagad2025chirality,
        title={Chirality in Action: Time-Aware Video Representation Learning by Latent Straightening},
        author={Bagad, Piyush and Zisserman, Andrew},
        journal={arXiv preprint arXiv:2509.08502},
        year={2025}
      }
      </code></pre>
      <!-- <button class="copy-button" data-clipboard-target="#code-block-article">Copy article</button> -->

      <pre><code id="code-block-inproceedings">
      @inproceedings{
            bagad2025chirality,
            title={Chirality in Action: Time-Aware Video Representation Learning by Latent Straightening},
            author={Bagad, Piyush and Zisserman, Andrew},
            booktitle={NeurIPS},
            year={2025}
      }
      </code></pre>
      <!-- <button class="copy-button" data-clipboard-target="#code-block-inproceedings">Copy conference</button> -->
    </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns">
      <div class="column">
        <h2 class="title is-4">üìô Related Work</h2>
        <div class="content has-text-justified">
          Please also consider looking at the following related papers that we were inspired by:
          <ul>
            <li>
              <a href="https://vision.cs.utexas.edu/projects/SeeAoT/">Seeing the Arrow of Time in Large Multimodal Models. </a> NeurIPS (2025).
            </li>
            <li>
              <a href="https://arxiv.org/abs/1909.09422">Retro-Actions: Learning ‚ÄòClose‚Äô by Time-Reversing ‚ÄòOpen‚Äô Videos.</a> ICCVW (2019).
            </li>
            <li>
              <a href="https://www.nature.com/articles/s41593-019-0377-4">Perceptual straightening of natural videos.</a> Nature Neuroscience (2019).
            </li>
        </div>
      </div>
    </div>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="#">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="#" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This website is based on the template from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>. We sincerely thank <a href="https://keunhong.com/">Keunhong Park</a> for developing and open-sourcing this template.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

<script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.8/clipboard.min.js"></script>
<script src="./static/js/copy_button.js"></script>

<script>
  const videos = [
    './assets/climb_up_down.mov',
    './assets/moving_up_down.mov',
    './assets/opening_closing_fridge.mov',
    './assets/pull_left_right.mov'
  ];
  
  const dinoVideos = [
    './assets/dino_trajectory-1.mp4',
    './assets/dino_trajectory-2.mp4',
    './assets/dino_trajectory-3.mp4',
    './assets/dino_trajectory-4.mp4',
    './assets/dino_trajectory-5.mp4',
    './assets/dino_trajectory-6.mp4',
    './assets/dino_trajectory-7.mp4',
    './assets/dino_trajectory-8.mp4',
    './assets/dino_trajectory-9.mp4',
    './assets/dino_trajectory-10.mp4',
    './assets/dino_trajectory-11.mp4',
    './assets/dino_trajectory-12.mp4',
    './assets/dino_trajectory-13.mp4',
    './assets/dino_trajectory-14.mp4',
    './assets/dino_trajectory-15.mp4'
  ];
  
  const qualImages = [
    './assets/qual-result-1.png',
    './assets/qual-result-2.png',
    './assets/qual-result-3.png'
  ];
  
  let currentVideoIndex = 0;
  let currentDinoVideoIndex = 0;
  let currentQualImageIndex = 0;
  
  function changeVideo(index) {
    const video = document.getElementById('carousel-video');
    const newSrc = videos[index];
    video.src = newSrc;
    video.load();
    video.playbackRate = 0.8;
    video.play();
  }
  
  function changeDinoVideo(index) {
    const video = document.getElementById('dino-carousel-video');
    const newSrc = dinoVideos[index];
    video.src = newSrc;
    video.load();
    video.playbackRate = 0.8;
    video.play();
  }
  
  function changeQualImage(index) {
    const image = document.getElementById('qual-carousel-image');
    const newSrc = qualImages[index];
    image.src = newSrc;
  }
  
  function nextVideo() {
    currentVideoIndex = (currentVideoIndex + 1) % videos.length;
    changeVideo(currentVideoIndex);
  }
  
  function previousVideo() {
    currentVideoIndex = (currentVideoIndex - 1 + videos.length) % videos.length;
    changeVideo(currentVideoIndex);
  }
  
  function nextDinoVideo() {
    currentDinoVideoIndex = (currentDinoVideoIndex + 1) % dinoVideos.length;
    changeDinoVideo(currentDinoVideoIndex);
  }
  
  function previousDinoVideo() {
    currentDinoVideoIndex = (currentDinoVideoIndex - 1 + dinoVideos.length) % dinoVideos.length;
    changeDinoVideo(currentDinoVideoIndex);
  }
  
  function nextQualImage() {
    currentQualImageIndex = (currentQualImageIndex + 1) % qualImages.length;
    changeQualImage(currentQualImageIndex);
  }
  
  function previousQualImage() {
    currentQualImageIndex = (currentQualImageIndex - 1 + qualImages.length) % qualImages.length;
    changeQualImage(currentQualImageIndex);
  }
  
  // Removed timer-based auto-play to allow videos to finish naturally
  
  // Start auto-play when page loads
  document.addEventListener('DOMContentLoaded', function() {
    // Advance only when each video ends
    const carouselVideo = document.getElementById('carousel-video');
    const dinoCarouselVideo = document.getElementById('dino-carousel-video');
    const modelVideo = document.getElementById('model-explainer-video');
    
    // Set playback speed
    if (carouselVideo) {
      carouselVideo.playbackRate = 0.8;
    }
    if (dinoCarouselVideo) {
      dinoCarouselVideo.playbackRate = 0.8;
    }

    carouselVideo.addEventListener('ended', nextVideo);
    dinoCarouselVideo.addEventListener('ended', nextDinoVideo);
    // Using native controls for model video; no custom handlers needed
  });
</script>

</body>
</html>
